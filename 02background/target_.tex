\section{Target Transformation in Univariate Time Series Forecasting}

Building on the ML regression modelling we established in the previous section, we can now address the topic of target transformation in univariate time series forecasting. For a forecasting task on a time series $Y = \{ Y_{t_i} \}_{i = 1, 2, \ldots}$, we try to devise a model $f$ that deterministicly produce predictions. We utilise all information available and formulate a modelling procedure goes as the following:
\begin{enumerate}
    \item Construct the corresponding training set $(\matr{y}_{train}, \matr{X}_{train})$, the validation set $(\matr{y}_{val}, \matr{X}_{val})$ and the test set $(\matr{y}_{test}, \matr{X}_{test})$ based on the objective (forecast horizon, gap), modelling configurations (sliding window size, validation size, test size).
    \item To find the optimal hyperparameter $\phi_{best}$, come up with a set of hyperparameters $\phi = \{\phi_i \}_{i = 1, 2, \ldots, k}$ and conduct model selection using the validation procedure and the validation set.
    \item Finally, test and report the test score of the model with the optimal hyperparameter $\phi_{best}$ using the test set.
\end{enumerate}
Observe that aside from our appointing the model $f$ and the hyperparameter set $\phi$, the model we obtain through the procedure rely heavily on the information contained within the dataset (training, validation and test) precisely because our formulation of the modelling procedure. In other words, the utility of our model depends heavily on the information set (dataset). This gives rise to a large number of studies trying to find more clever ways to process the information before putting it into the optimisation operation with the goal of boosting model performance. Such additional layer of operation on datasets in the modelling procedure is called \textit{data preprocessing}. Moreover, if the additional operation is performed only on the design matrix $\matr{X}$ such operations are referred to as \textit{feature engineering}\footnote{In regression operations, the columns in the design matrix are referred to as features or independent variables.}. When the operation is performed also on the target $\matr{y}$, it is called \textit{target transformation} or \textit{response transformation}\footnote{In regression operations, the target, being a column matrix, is also called the response variable or dependent variable}. In the rest of the section, we cover the technical notions of target transformation.

Let $\mathcal{T}: \mathbb{R}^M \longmapsto \mathbb{R}^M$ be an arbitrary target transformation that maps a $M$ dimensional vector to another $M$ dimensional vector. Throughout the modelling procedure, target transformation is to be embedded before every optimisation stated in equation \ref{eq: ml training}. Consider an arbitrary optimisation in the modelling procedure to be conducted with the information set $\mathcal{Y}_{t_k} = \{ y_{t_i} \}_{i = 1, 2, \ldots, k}$, we practice
\begin{equation*}
    \mathcal{Y}^{'}_{t_k} = \mathcal{T}(\mathcal{Y}_{t_k})
\end{equation*}
and use the transformed time series $\mathcal{Y}^{'}_{t_k}$ for the rest of the optimisation operation: given a hyperparameter $\phi_0$, we generate the training target $\matr{y}^{'}_{train}$, training design matrix $\matr{X}^{'}_{train}$ with $\mathcal{Y}^{'}_{t_k}$ and perform the optimisation
\begin{equation*}
    \theta(\phi_0)_0 = \arg_{\theta(\phi_0)} \max \mathcal{E}(\widehat{\matr{y}^{'}_{train}}, \matr{y}^{'}_{train}), \; \widehat{\matr{y}^{'}_{train}} = f(\theta(\phi_0); \matr{X}^{'}_{train}).
\end{equation*}
After the optimisation using the transformed time series, the usual next step is to make the validation target, validation design matrix and compute the fitness score of $\theta(\phi_0)_0$. However, there is an optional operation that might be needed depending on the nature of the transformation $\mathcal{T}$. If $\mathcal{T}$ changes the scale the original time series, i.e., $\mathcal{Y}_{t_k}$ and $\mathcal{Y}^{'}_{t_k}$ have different scale, then a \textit{back transformation} is needed to reverse the scale of the prediction generated by $f$ back to its original level. For example, an usual taret transformation used in financial studies is the \textit{log-difference} transformation. The log-difference operation is a simulation of the return we mentioned in equation \ref{eq: return}.
