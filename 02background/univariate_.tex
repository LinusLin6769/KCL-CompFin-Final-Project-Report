\section{Univariate Time Series Forecasting}
In this section, we discuss the topic of univariate time series forecasting.

\subsection{General Notions of Time Series Forecasting}
A time series is a sequential collection of random variables indexed by time. Suppose the random variables are of one dimension. The time series is considered univariate\footnote{If the random variables are of dimension higher than one, then the time series is considered multivariate. Datasets in such form are also referred to as panel data.}. In this section, the discussions fall within the context of univariate time series forecasting. Let $Y = \{Y_{t_i} \}_{i = 1, 2, \ldots, n}$ be an univariate time series with $ Y_{t_i} \in \mathbb{R}, \; \forall i, \; n \in \mathbb{N}$ and $t = \{t_i \}_{i = 1, 2, \ldots, n}$ being the set of time indices of $Y$ (also referred to as the set of timestamps). Let $\mathcal{Y}_{t_k}$ be the largest information set about $Y$ that is accessible to the model at time point $t_k \in t$, e.g., we might have the observations of $Y$ being known ($\{Y_{t_j} = y_{t_j}\}_{j = 1, 2, \ldots, k } \subset \mathcal{Y}_{t_k}$). Then in the context of modelling, for an unknown (random) target $Y_{t_s}$ with $s > k, \; s \in \mathbb{N}$, we can articulate the notion of forecasting as the following:
\begin{displayquote}
    Forecasting the value $Y_{t_s}$ at time point $t_k$ is to find a function $f(\mathcal{Y}_{t_k}) = \widehat{Y_{t_s}}$, such that $\widehat{Y_{t_s}}$ is a good estimation of $Y_{t_s}$.
\end{displayquote}
We can develop all sorts of functions $f$ for such forecasting objectives. Functions devised to serve the objective are referred to as models. In a general sense, the notion of modelling refers to the methodologies of devising a model that serves the objective well. In particular, for any arbitrary pair of $t_k$ and $t_s$, we want to have a model $f(\mathcal{Y}_{t_k})$ such that it gives us a reliable estimation of $Y_{t_s}$.

\subsubsection{Gap}
Notice that a modelling objective is parameterised by the pair of time indices $(t_k, t_s)$. The timestamp $t_k$ directly affects the information set $\mathcal{Y}_{t_k}$ and thus determines the information the model $f$ can utilise. The timestamp $t_s$, on the other hand, controls how far in the future we are forecasting. If the gap between the two timestamps is big, the model is asked to forecast further into the future. If the gap between the two timestamps is small, then the objective might be considered easier because we are only trying to look a tiny step ahead into the future. In order to better communicate and characterise the forecasting objective, we formalise this gap between the pair of timestamps as the \textit{gap} and denote it as $\tau$:
\begin{equation*}
    \tau = t_s - t_k
\end{equation*}
With such a notion of the gap, we can then articulate the forecasting objective as $\tau$ ahead forecasting. $\tau$ takes the format of the timestamps. Depending on the format of the timestamps, the objective can be one-step ahead forecasting or one-year ahead forecasting. We will go into topics concerning timestamps in one of the upcoming paragraphs.

\subsubsection{Forecast Horizon}
Observe that the target we have in the previous forecasting objective $Y_{t_s}$ is a single value in the future. It is possible to generalise the target and have multiple targets in the future. The number of targets we try to forecast is called the \textit{forecast horizon}. Having a forecast horizon equal to one is to forecast one value into the future, and having a forecast horizon equal to five is to forecast all five values into the future. To put it formally, let $\langle \cdot \rangle$ be a counting operation that counts the number of elements for a finite set, and let a task have a finite collection of unknown values $Y_S = \{ Y_{s_1}, Y_{s_2}, \cdots \}_{s_1, s_2, \ldots \in t, \; s_1, s_2, \ldots > t_k}$. Then the forecast horizon of such a task is denoted as
\begin{equation*}
    H = \langle Y_S \rangle
\end{equation*}

\subsubsection{Timestamps}
In a time series, the time index of a random variable carries information about the time point in which the random variable lives in the time domain. In some sense, the timestamps mark the `location' of the random variables on a timeline. For example, a monthly revenue time series $Y$ in an arbitrary year can have the set of months in a year as its timestamp set and be denoted as $Y = \{ Y_{Jan}, Y_{Feb}, Y_{Mar}, \cdots, Y_{Dec} \}$. The time indices also tell us the time-relevance (geological relationship on a timeline) of the random variables among each other. In fact, the time-relevance of the random variables in a time series plays a crucial role in time series analysis. We often have to perform mathematical operations involving such relationships. An example is our coming up with the gap measurement we addressed in the previous paragraph. Another example is the calculation of the relative growth of the time series. The need for these math operations pushes modellers to devise innovative ways to define the timestamps because we cannot easily perform calculations on notations like $September$ or $Friday$. To put it in math terms, what we often do is to have a mapping from physical timestamps to the real number line (or a subset of the real number set, say, the natural number set) and use the target set of this mapping as the timestamp set for math operations. In the next paragraph, we discuss some examples of such mapping.

Take the previous monthly revenue time series as an example; one simplest way is to index the time series chronologically with natural numbers $\{ 1, 2, 3, \cdots, 12 \}$. The new index system allows for mathematical operations on the timestamps, such as addition. The objective of two-month ahead forecasting can be considered as two-step ahead forecasting with $\tau = 2$. Three-month moving average of the time series can now be of a generalised form of a three-step moving average. Another good example is the financial studies of stochastic processes, in which we often use the non-negative real line and adopt an annual scale, i.e., the starting point of the time series is indexed as $0$, one month after that is indexed $0.0833$, one-year time point is indexed $1.0$, and so on. This is particularly useful when we expand the time series studies using Stochastic Differential Equations (SDE). Let the SDE of a stochastic process $dY_t$ be given as
\begin{equation*}
    \frac{dY_t}{Y_t} = \mu_t dt + \sigma_t dW_t, \quad dW_t \sim N(0, dt).
\end{equation*}
$dt$ in the drift term is now properly defined as a real number on which we can do all sorts of math operations (observe how $dW_t$ is defined as a Brownian Motion that follows a Gaussian distribution with variance $dt$).

\subsubsection{Time Heterogeneity}
There are cases where the timestamps of the time series are not identically spaced between consecutive random variables. The previous example of monthly revenue in a year is one of them due to the months having different durations. Time series as such is technically referred to as being \textit{time heterogeneous}\footnote{Time heterogeneity is common in financial time series due to the nature of how financial markets work. For example, most markets are open only during working hours on working days. Another example is the high-frequency financial time series (see Dacarogna et al. \citeyear{genccay2001introduction}). \textit{Time homogeneity} is the counterpart of time heterogeneity, specifying time series which have equally spaced timestamps}. Time heterogeneity can be an interesting source of information carried by the time series but can also be a major issue in time series studies. The following paragraph presents a common problem caused by time heterogeneity.

Calculating measurements related to the unit of time can be a problem with time heterogeneous time series. One common example of such measurement is the return used in finance. Return measures the relative change of the price (or, say, value) over a period of time with respect to its initial level. Several definitions can be drawn to the notion of return, but we will look at the simplest one as they all exhibit the same relationship with time heterogeneity. Let $Y = \{ Y_{t_i} \}_{i = 1, 2, \ldots n}, \; n \in \mathbb{N}$ now be a time series of the price movement of an asset over time, and the time index set being $t = \{ t_i \}_{i = 1, 2, \ldots, n}$. Define the corresponding net return measure as
\begin{equation}\label{eq: return}
R_{t_i} = \frac{Y_{t_i} - Y_{t_{i-1}}}{Y_{t_{i-1}}}, \quad i \in \{2, 3, \cdots, n \}.
\end{equation}
The net return $R_{t_i}$ for some $i$ is thus the relative price change of $Y_{t_i}$ over the time period $t_i - t_{i-1}$ with respect to $Y_{t_{i-1}}$. Let $R = \{ R_{t_i} \}_{i = 2, 3, \ldots, n}$ be the time series of one-step net returns derived from $Y$. If $t$ is equally spaced by, say, a day, the time series $Y$ is time-homogeneous. The time series $R$ we calculated is a time series of daily net return of $Y$. Nevertheless, in the case where $t$ is not equally spaced, the time series $Y$ is time heterogeneous. Then we no longer know the period of the net returns $R$ we calculated from the time series $Y$. This is an example of how time heterogeneity can complicate time series analyses. Time heterogeneity in finance has been studied a lot, especially with the popularisation of electronic systems. See Dacarogna et al. (\citeyear{genccay2001introduction}) for more information.

\subsection{Machine Learning Regression Modelling}
In this section, we further discuss univariate time series modelling and address how we approach the articulated forecasting objective with Machine Learning (ML) regression modelling. In addition to addressing the approach, we also provide relevant statistical notions that can be seen as the underlying theoretical foundation of the standard machine learning procedure.

Recall the forecasting objective is to come up with a model $f$ capable of generating `good' estimations of some target $Y_{t_s}$ at time $t_k$ using the provided information $\mathcal{Y}_{t_k}$. We will see in the later paragraphs that this is very similar to the process of solving for a Quasi-Maximum Likelihood Estimation (QMLE) in a statistical sense (White (\citeyear{white1982maximum})). In the context of modelling, the procedure is normally to gather the available information and formulate it into an optimisation problem: we define a fitness measure (like the `L' in QMLE), which should be a function of the estimates $f(\mathcal{Y}_{t_k})$ and the target, and we then optimise the fitness as an objective function with respect to the model $f$ in some algorithmic way (the `M' in QMLE). The final output of such a procedure will be a model (function $f$) that optimally serves our objective (the `E' in QMLE). In the remainder of this section, we discuss the general framework of how the procedure works.

\subsubsection{The Sliding Window, Design Matrix and Target}
In regression problems, the design matrix and the target are made from accessible information and formulated for a modelling environment. In this paragraph, we discuss how to develop the design matrix and target in univariate time series modelling. Recall that the forecasting objective is to estimate $Y_{t_s}$ at time point $t_k$, with the gap $\tau = t_s - t_k > 0$ and the model is only able to utilise the accessible information set $\mathcal{Y}_{t_k}$. This dataset is called the \textit{training set}. For our modelling problem, we can only use what is provided in our training set $\mathcal{Y}_{t_k}$. Without loss of generality, for the target $Y_{t_s}$, the information set accessible is $\mathcal{Y}_{t_s - \tau}$. In the event we have a one dimensional time series, $\mathcal{Y}_{t_s-\tau}$ is simply the collection of realised values of $Y$ until time point $t_s-\tau = t_k$, namely
\begin{equation*}
    \mathcal{Y}_{t_s-\tau} = \{ Y_{t_s-\tau} = y_{t_s-\tau}, Y_{t_{s-1}-\tau} = y_{t_{s-1}-\tau}, Y_{t_{s-2}-\tau} = y_{t_{s-2}-\tau}, \cdots, Y_{t_1} = y_{t_1} \}.
\end{equation*}
To make use of this long list of past observations, the idea is to create a sandbox in which we simulate the model making predictions. The environment in which the model makes a prediction is simply a mapping from the information it can use to a target. We do this by using the methodology called the \textit{sliding window}. The sliding window is parameterised by a single constant parameter $\lambda, \; \lambda \in \mathbb{N}, \; \lambda > 2$ that controls the width of the window. The decision of $\lambda$ should take into account modelling configurations including the size of our training set, forecast horizon, gap, and information we want the model to use in a single prediction task. Once $\lambda$ is decided, we move the window chronologically throughout $\mathcal{Y}_{t_s -\tau}$. For every step of the window, we create an independent prediction instance for the model using the values contain within the window - the final value in a single window is the target for horizon one forecasting and the rest of the values are potentially accessible to the model for its prediction. We then have numerous such forecasting instances for the model from which we can foster the whole sandbox. The sandbox consists of the design matrix $\matr{X}$ and the target $\matr{y}$. They can be formulated as
\begin{equation*}
    \matr{y} = \begin{bmatrix}
        y_{t_\lambda}       \\
        y_{t_{\lambda + 1}} \\
        \cdot               \\
        \cdot               \\
        y_{t_{k-1}}         \\
        y_{t_k}             \\
    \end{bmatrix}
    , \quad
    \matr{X} = \begin{bmatrix}
        y_{t_{\lambda} - \tau}   & y_{t_{\lambda-1} - \tau} & \cdots & y_{t_{1}} \\
        y_{t_{\lambda+1} - \tau} & y_{t_{\lambda} - \tau}   & \cdots & y_{t_{2}} \\
        \cdot                    & \cdot                    & \cdots & \cdot     \\
        \cdot                    & \cdot                    & \cdots & \cdot     \\
        y_{t_{k-1} - \tau}       & y_{t_{k-2} - \tau}       & \cdots & y_{t_{k - \lambda} - \tau}     \\
        y_{t_{k} - \tau}         & y_{t_{k-1} - \tau}       & \cdots & y_{t_{k - \lambda + 1} - \tau} \\
    \end{bmatrix}.
\end{equation*}
The target $\matr{y}$ is a $k-\lambda+1$ by $1$ column matrix and the design matrix $\matr{X}$ is of $k-\lambda+1$ by $\lambda$. In this training environment, the model has $k-\lambda + 1$ predictions to make, each being to use a row vector in $\matr{X}$, denoted as $\matr{X}_i, \; i \in \{1, 2, \cdots, k-\lambda+1 \}$ and estimate the corresponding row element in $\matr{y}$, denoted as $\matr{y}_i$. The idea is to find a model that best maps the rows in $\matr{X}$ to elements in $\matr{y}$ in general, and we say this is our best model $f$ that serves the objective of a $\tau$ ahead forecasting task given the time series we have.

\subsubsection{The Number of Lags}
One remark regarding the design matrix is the notion of the \textit{number of lags}. The number of lags describes how many latest observations the model is allowed to use for a single forecasting task, i.e., the number of columns in $\matr{X}$. Its naming originates in autoregressive estimation in time series studies. Such estimation aims at finding the optimal order of the autoregressive feature of a time series. This is equivalent to finding the optimal number of latest observations the model should use for a single prediction. Typically, such autoregressive order is referred to as the \textit{lag}.

\subsubsection{Modelling}
We start by describing what a model is in more detail. Given the design matrix $\matr{X}$ and target $\matr{y}$ we made in the previous section, the forecasting objective is now transformed into coming up with a model $f$ that best maps the rows in $\matr{X}$ to the corresponding element in $\matr{y}$.

Consider $f$ as an arbitrary machine learning regression model with a known structure. Knowing the structure of $f$ implies we know the structure of its parameter set and how $f$ maps $\matr{X}_i$ to $\matr{y}_i$ for some $i$. Let $\theta$ be the mapping that generates the parameters that go into $f$. The output of $\theta$ is parameterised by its input set; let it be $\phi$. Parameter set like $\phi$ is called the \textit{hyperparameters} - it characterises the parameters of $f$. The model $f$ in our forecasting objective can thus be noted as
\begin{equation*}
    f(\theta(\phi); \matr{X}) = \widehat{\matr{y}} \sim \matr{y}.
\end{equation*}
To better measure the performance of such an estimation, we define a fitness function $\mathcal{E}$. The fitness function takes the estimations generated by $f$ and returns a real number signifying how well they fit the target. We can finally formulate our modelling objective as
\begin{equation}\label{eq: ml training}
    \arg_{\theta(\phi)} \max \mathcal{E}(\widehat{\matr{y}}, \matr{y}), \quad \widehat{\matr{y}} = f(\theta(\phi); \matr{X}).
\end{equation}
This process is what we call \textit{model training}.

To give an example, if $f$ is a simple linear regression model, then we know $\theta$ gives a tuple of real numbers (known as weights) which the model uses to generate a linear combination of its inputs; in this case, a row matrix $\matr{X}_i$. Specifications of $\theta$ are then controlled by its input $\phi$, e.g., whether there is an intercept term or the number of elements of the tuple\footnote{Note that the number of elements in the tuple depends on the number of lags we have in making the design matrix $\matr{X}$, i.e., $\lambda \in \phi$.}. Not knowing the exact values of $\theta$ (and certainly $\phi$ as well) describes the state of the model $f$ being untrained. Then the modelling objective is to find $\phi$ and $\theta$ such that $\mathcal{E}(f(\theta(\phi); \matr{X}), \matr{y})$ is maximised.

\subsubsection{The Statistical Resemblance}
Analogously, training a machine learning model can be put into statistical terms. In particular, it resembles the Quasi-Maximum Likelihood Estimation (QMLE) process with some minor tweaks. Consider our objective with $Y$, but with the elements following an arbitrary distribution $\mathcal{D}$ characterised by $\theta$, i.e., $Y_{t_i} \sim \mathcal{D}(\theta), \; \forall \; t_i \in t$\footnote{The `Quasi-' simply means we do not know whether the distribution $\mathcal{D}$ is Gaussian or not (see White \citeyear{white1982maximum}).}. Consider an artibrary pair of timestamps $(t_k, t_s)$ and let $\mathcal{F}_{Y_{t_s} | \mathcal{Y}_{t_k}}(\cdot; \theta)$ be the conditional joint probability density function (pdf) of the random variable $Y_{t_s}$ given $\mathcal{Y}_{t_k}$ and $\theta$. Then the expression
\begin{equation*}
    \mathcal{F}_{Y_{t_s} | \mathcal{Y}_{t_k}}(y_{t_s}| \mathcal{Y}_{t_k} ; \theta)
\end{equation*}
describes the probability of observing $Y_{t_s} = y_{t_s}$ conditional on the past observations and parameter $\theta$. Let the value of $Y_{t_s} = y_{t_s}$ be given, then the objective of QMLE is to find the $\theta$ conditional on observing $\mathcal{Y}_{t_k}$, under which $y_{t_s}$ is most likely to be observed. We can formalise such objectives as
\begin{equation*}
    \arg_{\theta} \max \mathcal{F}_{Y_{t_s} | \mathcal{Y}_{t_k}}(\theta ; y_{t_s} | \mathcal{Y}_{t_k}).
\end{equation*}
Notice how the variable is now $\theta$ while the observations are given. The new objective function to be optimised is called the \textit{likelihood function}, denoted as $\mathcal{L}(\theta)$. $\mathcal{L}(\theta)$ is essentially still a pdf. It returns the probability of observing $y_{t_s}$ conditional on $\mathcal{Y}_{t_k}$ with the parameter $\theta$. Maximising such likelihood function with respect to $\theta$ is thus equivalent to finding a $\theta$ dictating the generating mechanism of the random variable $Y_{t_s}$ such that it is most likely to be observed as $y_{t_s}$.

Training a machine learning model bears some resemblance to QMLE. Training a model aims to find a mechanism to reproduce the target as the observation. At the same time, QMLE tries to find the set of parameters characterising the distribution of the target variable conditional on its past realisations. Both methodologies tackle the problem with an optimisation framework involving an objective function: QMLE utilises the probability density function (pdf) that comes with a random variable with a known distribution. ML modelling devises a fitness function to evaluate the goodness of the estimation. The outcome of training an ML model is to have a deterministic function that generates the target and serves the purpose of forecasting. On the other hand, QMLE yields a set of parameters dictating the underlying distribution of the target variables, i.e., you end up having a recipe to construct a probabilistic distribution not deterministicly generate a target value. We hope this section contributes to a better theoretical understanding of ML modelling in terms of statistical analysis.

\subsubsection{Model Selection with Validation}
In the phase of training a model using $\mathcal{Y}_{t_k}$, notice that we have to first have the hyperparameters $\phi$ before we actually starting searching for optimal $\theta$. Such process of finding $\phi$ is called \textit{model selection} or \textit{hyperparameter tuning}. The way we approach model selection starts by selecting a subset of the training set with respect to the row and call it the \textit{validation set}\footnote{The choice of validation set is usually a proportion of the latest instances from the training set, say, the latest ten instances or the latest ten percents of instances of the training set.}. Let the size of the validation set be $v, \; v \in \mathbb{N}, \; v \ll k - \lambda + 1$\footnote{The decision of $v$ depends on the size of our training dataset. The bigger $v$ we wish to have, the less data is left for the training.}. We then try (validate) whether a $\phi$ candidate works fine using the validation set. For a given hyperparameter candidate $\phi_0$, we know the exact structure of $\theta(\phi_0)$ such that we can run the optimisation regime, i.e., model training using equation \ref{eq: ml training}. Then for $\phi_0$, we do the \textit{validation} illustrated as algorithm \ref{alg: validation}.
\begin{algorithm}[H]
    \caption{Validation}\label{alg: validation}
    \begin{algorithmic}
    \State{Let $L$ be an empty list.}
    \State{Let $s = k - \lambda + 1 - v$ be the first index of the validation set.}
    \For{$ i \in \{0, 1, 2, \cdots, v-1 \}$}
        \State{$\matr{y}_{train}  \leftarrow \{\matr{y}_i \}_{i = 1, 2, \ldots, s - 1 + i}$ Use information before the validation point.}
        \State{$\matr{X}_{train}  \leftarrow \{\matr{X}_i \}_{i = 1, 2, \ldots, s - 1 + i}$ Use information before the validation point.}
        \State{Do training: $\theta(\phi_0)_0 \leftarrow \arg_{\theta(\phi_0)} \max \mathcal{E}(\widehat{\matr{y}_{train}}, \matr{y}_{train}), \; \widehat{\matr{y}_{train}} = f(\theta(\phi_0); \matr{X}_{train})$}
        \State{$\matr{y}_{val}    \leftarrow \matr{y}_{ s + i}$ This is the validation instance in $\matr{y}$.}
        \State{$\matr{X}_{val}    \leftarrow \matr{X}_{ s + i}$ This is the validation instance in $\matr{X}$.}
        \State{Compute fitness score for the $\theta(\phi_0)_0$: $\nu \leftarrow \mathcal{E}(f(\theta(\phi_0)_0; \matr{X}_{val}), \matr{y}_{val})$}
        \State{Store the fitness score $\nu$ in list $L$}
    \EndFor
    \State{Let $V_{\phi_0} \equiv \frac{1}{v}\sum{L}$ be the mean of the scores stored in list $L$.}
    \end{algorithmic}
\end{algorithm}
The result for validating a single hyperparameter $\phi_0$ is the validation score $V_{\phi_0}$ we compute in the final step of the algorithm. Normally, we will come up with a set of $k$ candidate hyperparameters $\theta = \{ \theta_i \}_{i = 1, 2, \ldots, k}, \; k \in \mathbb{N}$ and repeat such validation process $k$ times and yield $k$ validation scores $\{ V_{\phi_i} \}, \; \forall i \in \{1, 2, \cdots, k \}$. We then take the hyperparameter set with the most preferable fitness score
\begin{equation*} 
    \phi_{best} = \arg_{\phi_i} \max \{ V_{\phi_i} \}_{i = 1, 2, \ldots, k}
\end{equation*}
and say $\phi_{best}$ is the optimal hyperparameter set to use in our modelling problem. The set of $k$ candidate hyperparameters $\phi$ is referred to as the \textit{hyperparameter space}. And hyperparameter tuning is the act of searching for an optimal hyperparameter in the hyperparameter space.

\subsubsection{Model Evaluation - Testing}
In the event we wish to conclude the model's performance using some measure, this is called \textit{model evaluation} or \textit{testing the model}. Testing is done in a similar manner as the validation we addressed in model selection (notice how we use a validation score to measure the performance of a model). In some cases, we could simply use the validation score of the best performing hyperparameter $V_{\phi_{best}}$ because this score is also a concluding measure of our model performance. However, if we want to actually simulate the situation in which our model is put into production, then we can simply take another segment from the training data (a proportion of latest observations) and perform a single round of validation, except for this time, it is not validation anymore. We refer this to as model evalutaion and the dataset used for it as the \textit{test set}. Let $\kappa$ denote the size of the test dataset. Then we have to push the validation set and let the latest $\kappa$ instances in the training set be the test set. Our formation of the sandbox is now turned and renamed into
\begin{align*}
    \matr{y}_{train} &=
    \begin{bmatrix}
        y_{t_\lambda}            \\
        y_{t_{\lambda + 1}}      \\
        \cdot                    \\
        \cdot                    \\
        y_{t_{k-1 - v - \kappa}} \\
        y_{t_{k - v - \kappa}}
    \end{bmatrix}
    , \quad
    \matr{X}_{train} =
    \begin{bmatrix}
        y_{t_{\lambda} - \tau}    & y_{t_{\lambda-1} - \tau}  & \cdots & y_{t_{1}}                         \\
        y_{t_{\lambda+1} - \tau}  & y_{t_{\lambda} - \tau}    & \cdots & y_{t_{2}}                         \\
        \cdot                     & \cdot                     & \cdots & \cdot                             \\
        \cdot                     & \cdot                     & \cdots & \cdot                             \\
        y_{t_{k-1-v-\kappa}-\tau} & y_{t_{k-2-v-\kappa}-\tau} & \cdots & y_{t_{k-\lambda-v-\kappa}-\tau}   \\
        y_{t_{k-v-\kappa}-\tau}   & y_{t_{k-1-v-\kappa}-\tau} & \cdots & y_{t_{k-\lambda+1-v-\kappa}-\tau}
    \end{bmatrix}\\
    \matr{y}_{val} &=
    \begin{bmatrix}
        y_{t_{k - v - \kappa + 1}}  \\
        y_{t_{k - v - \kappa + 2}}  \\
        \cdot                       \\
        \cdot                       \\
        y_{t_{k - v - 1}}           \\
        y_{t_{k - v}}            
    \end{bmatrix}
    , \quad
    \matr{X}_{val} =
    \begin{bmatrix}
        y_{t_{k-v-\kappa+1}-\tau} & y_{t_{k-v-\kappa}-\tau}   & \cdots & y_{t_{k-\lambda+1-v-\kappa+1}-\tau} \\
        y_{t_{k-v-\kappa+2}-\tau} & y_{t_{k-v-\kappa+1}-\tau} & \cdots & y_{t_{k-\lambda+1-v-\kappa+2}-\tau} \\
        \cdot                     & \cdot                     & \cdots & \cdot                               \\
        \cdot                     & \cdot                     & \cdots & \cdot                               \\
        y_{t_{k-v-1}-\tau}        & y_{t_{k-v-2} - \tau}      & \cdots & y_{t_{k-\lambda+1-v-1}-\tau}        \\
        y_{t_{k-v}-\tau}          & y_{t_{k-v-1} - \tau}      & \cdots & y_{t_{k-\lambda+1-v}-\tau} 
    \end{bmatrix}\\
    \matr{y}_{test} &=
    \begin{bmatrix}
        y_{t_{k - v + 1}}  \\
        y_{t_{k - v + 2}}  \\
        \cdot              \\
        \cdot              \\
        y_{t_{k - 1}}      \\
        y_{t_k}
    \end{bmatrix}
    , \quad
    \matr{X}_{test} =
    \begin{bmatrix}
        y_{t_{k-v+1} - \tau}     & y_{t_{k-v} - \tau}       & \cdots & y_{t_{k-\lambda+1-v+1}}      \\
        y_{t_{\lambda+1} - \tau} & y_{t_{\lambda} - \tau}   & \cdots & y_{t_{k-\lambda+1-v+2}}      \\
        \cdot                    & \cdot                    & \cdots & \cdot                        \\
        \cdot                    & \cdot                    & \cdots & \cdot                        \\
        y_{t_{k-1} - \tau}       & y_{t_{k-2} - \tau}       & \cdots & y_{t_{k-\lambda+1-1} - \tau} \\
        y_{t_{k} - \tau}         & y_{t_{k-1} - \tau}       & \cdots & y_{t_{k-\lambda+1} - \tau} 
    \end{bmatrix}
\end{align*}
To sum up, the \textit{training set} now consists of $k - \lambda + 1 - v - \kappa$ instances, the \textit{validation set} consists of $v$ instances and the \textit{test set} consists of $\kappa$ instances. The three sets take up all the information we have in our accessible information set $\mathcal{Y}_{t_k}$. Given we already have the best performing hyperparameter $V_{\phi_{best}}$, we can simply do a single round of validation described in algorithm \ref{alg: validation} using the test set and come up with a score that concludes the model performance. Such score is referred to as the \textit{test score}. And that concludes a typical modelling operation.
