\chapter{Literature Review}\label{ch: literature review}

Target transformation techniques have been researched as part of the modelling process over the past sixty years. The same applies to the analyses of the underlying mechanisms in financial dynamics. This paper builds on both research directions and analyses the Directional Change (DC) intrinsic time framework as a target transformation technique in time series forecasting. To the best of our knowledge, as this paper was written, we are unaware of any other attempt to bring these two methodologies together. In this chapter, we cover the most relevant works in both realms. We first review some target transformation developments in Section \ref{sec: target transformation} and then look at some of the advancements in the Directional Change intrinsic time framework in Section \ref{sec: dc}.

\section{Target Transformation}\label{sec: target transformation}

In most cases, time series analyses yielded from modelling are justified conditionally on the assumptions made by the models about the data. If the assumptions are not met by the original data, applying some transformation (often non-linear) to the data can help generate these conditions. This has led to various transformation techniques for these types of purposes. In this section, we go through some important assumptions made by the models and some of the most popular corresponding transformations found in the literature.

Homoscedasticity condition\footnote{The homoscedasticity (homogeneity of variances) condition requires the variances of different subsets of the sample to be the same. In the case of time series modelling, it is equivalent to requiring a constant variance throughout time.} is one of the most common assumptions for models involving statistical inference. As a reference to the analysis of variance, M. S. Bartlett (\citeyear{10.2307/3001536}) provided one of the earliest summaries of transformations on raw data addressing this. He covered parametric transformations used in stabilising the variance of modelling error, especially for Poisson and Binomial distributed variables where the variance is a known function of the mean. He discussed, both theoretically and empirically, some of the optimal scales and families of transformations to choose from given different prerequisites. His work showed that modelling tasks do benefit from suitable transformations.

Another common assumption made by the models is normality. Many statistical inference methods assume the variable of interest is normally distributed, including t-test, Analysis of Variance (ANOVA) and Linear Regression. Therefore, it is of paramount importance to transform the data in a way that such conditions are met, if possible. Box \& Cox (\citeyear{box1964analysis}) made a major contribution in this regard by proposing the well-known Box-Cox transformation. The Box-Cox transformation includes both power and logarithmic transformations. It aims at achieving normality of the observations and has been popular in developing modelling methodologies ever since (see Atkinson et al. \citeyear{atkinson2021box} and Osborne \citeyear{osborne2010improving}). An example of applying Box-Cox and achieving better model performance was given in C. Bergmeier et al. (\citeyear{bergmeir2016bagging}). Combined with the widespread exponential smoothing (ETS) forecasting method and the bootstrap aggregation (bagging) technique in machine learning, they proposed a bagging exponential smoothing method using STL decomposition and Box-Cox transformation. The proposed method was tested on the M3 competition dataset and achieved better results than all the original M3 participants (see Makridakis et al. \citeyear{makridakis2000m3} for the M3 competition).

The method published by Box \& Cox, however, is only valid for positive real values. Modifications of the Box-Cox transformation have thus been proposed to address this constraint. A significant extension was proposed by Bickel \& Doksum (\citeyear{bickel1981analysis}). They embedded a sign function to the power transformation such that the transformation function covers the whole real line. Nevertheless, this modification has its shortcomings: it was shown by Yeo \& Johnson (\citeyear{10.1093/Biomet/87.4.954}) that Bickel \& Doksum's modified version of the transformation handles skewed distribution poorly. Yeo \& Johnson pointed out the reason being the signed power transformation was designed primarily for handling kurtosis, thus losing its edge concerning skewness. Following up, Yeo \& Johnson proposed a new version of the power transformation in the same publication (\citeyear{10.1093/Biomet/87.4.954}). Their transformation is a generalised version of the Box-Cox transformation and approximates normality while being well-defined on the real line and inducing appropriate symmetricity.

In the previous paragraphs we have described transformation techniques that can be used to satisfy mathematical and statistical conditions assumed by the models. Meeting these assumptions improves the robustness of the conclusions drawn by the modelling results. On a higher level, one can say that the transformations help the models to get better at `learning' the problem such that they generate more robust outputs. In the upcoming paragraphs, we take on this perspective of treating the transformation techniques as helpers in terms of the learning process of the models and look at some transformation techniques very different from what was covered previously.

Decomposition methods constitute a significant category of techniques that can be used to transform the target in order to help the models learn from the target variable. These methods decompose the mixture of information contained within the observation into patterns, trends, cycles, or other dynamics that are easier to model, i.e., easier for models to learn from. A good example of such methods includes the large number of studies on Fourier-styled transformations in time series modelling\footnote{Such studies are also called spectrum analysis.} (see Kay \& Marple \citeyear{kay1981spectrum} and Bloomfield \citeyear{bloomfield2004fourier}). Typical Fourier methodologies build on transforming the observations sampled from the time domain into the frequency domain and decomposing them into more informative signals. Out of the great history and advancements of spectrum analysis, we selectively provide a brief overview of some relevant methodologies in the context of target transformation.

Building on Fourier methods, a novel type of transformations operates in the time-frequency domain, i.e., these methods generate time-frequency representations of the observations. The empirical mode decomposition (EMD) proposed by Huang et al. (\citeyear{huang1998empirical}) is one of such key methodologies. EMD decomposes the original time series into `intrinsic mode functions' (IMF). The IMF's carry information of the underlying structures contained in the observations and can then be used for modelling tasks. The family of wavelet transform methods constitutes another class of methods that operates in the time-frequency domain (see Daubechies \citeyear{daubechies1992ten} and Percival \& Walden \citeyear{percival2000wavelet}). Shensa et al. \citeyear{shensa1992discrete} first proposed and provided a framework for the discrete wavelet transform (DWT), which belongs to the wavelet transform family. DWT filters the original time series in several folds and yields a denoised version of the observation. The information carried by the transformed time series is then more clear and possibly easier to learn. The rationale of using such techniques as target transformation in time series modelling is to provide the models with a more informative, e.g., less noisy, dataset to learn from. We hope this extra procedure helps with the ultimate goal to produce a better trained (learned) model.

Another important family of decomposition methods consists of statistical-related methods. These decomposition methods generally consider the time series as a mixture of three components: seasonal, trend and remainder. They are often used to filter different information contained within the time series (see Wang, Smith, \& Hyndman \citeyear{wang2006characteristic}). For a real-world example, monthly unemployment data are usually presented after removing the seasonality. The resulting time series is hence more indicative of the variation of the general economy instead of seasonal disturbance (see Chapter 3.2 in Hyndman \& Athanasopoulos \citeyear{forecastingprincipleandpractice}). The STL decomposition proposed by Cleveland et al. \citeyear{cleveland1990stl} has been a robust method for decomposition into seasonal and trend signals. The abbreviation stands for Seasonal and Trend decomposition using Loess. STL considers a time series as a sum (additive) or product (multiplicative) of the seasonal, trend and remainder components. STL is flexible and applicable to many use cases as it can handle any type of seasonality. Its flexibility also resides in its allowance for the user to have control over the time-varying seasonal component and smoothness of the trend cycle. The X-11 method and the Seasonal Extraction in ARIMA Time Series (SEATS) procedure are time series models that rely heavily on seasonal and trend decompositions. They have had many variants and are favoured by official statistical agencies around the world (see Dagum \& Bianconcini \citeyear{dagum2016seasonal})\footnote{X-11 was initially developed by the US Census Bureau, and SEATS was created by the Bank of Spain.}. One of the state-of-the-art variants of this family is the X-13ARIMA-SEATS method produced, distributed and maintained by the US Census Bureau (see US Census Bureau \citeyear{x13arimaseatsmanual} and Monsell \& Blakely \citeyear{monsell2013x}). It inherits powerful features from X-11, SEATS, and ARIMA methodologies while specialising in seasonal adjustment in extensive time series modelling. The model is conveniently accessible online\footnotemark{}.
\footnotetext{A webpage demonstration of the model is accessible on \url{http://www.seasonal.website/}; the open-source implementation of the model can be found in the \verb+seasonal+ package in R, and a distributed version can be found in the US Census Bureau website \url{https://www.census.gov/data/software/x13as.X-13ARIMA-SEATS.html}.}

\section{Directional Change intrinsic time framework}\label{sec: dc}

The technical core of the Directional Change (DC) intrinsic time framework is quite simple; it is an algorithm (the DC dissection) that samples a time series and yields a new time series, which is a subset of the original observations. By analysing the properties of the resulting time series, it has been found that despite the simplicity of this algorithm, it provides powerful perspectives for looking at market dynamics in the time domain. In this section, we first look at critical works that contribute to the advancements of the DC intrinsic time framework. Then we cover some applications that further developed the framework's value by trying to harness its potential.

Like the developments of many frameworks, the DC intrinsic time framework started out being simple and has developed over time. Guillaume et al. (\citeyear{guillaume1997bird}) first published the Directional Change dissection algorithm. The algorithm was presented and used to generate a set of measurements (statistics, variables), from which the authors presented a set of stylised facts found empirically in the spot intra-daily foreign exchange (FX) markets. These stylised facts shed new light on our understanding of market dynamics, especially concerning micro-structure topics, including time-heterogeneity, price formation, market efficiency, liquidity, and both the modelling and the learning process of the market. A little more than a decade later, Tsang formalised the definition of a Directional Change in Tsang (\citeyear{tsang2010directional}), and Glattfelder et al. (\citeyear{glattfelder2011patterns}) discovered a set of twelve scaling laws derived from the DC sampling algorithm. The discovery of the laws added a theoretical foundation to the DC dissection algorithm as it was shown that the output time series carries not only qualitative information (stylised facts) but also interesting quantitative properties. As the DC dissection algorithm's ability to extract information has been studied, it has given rise to the methodology becoming a framework (see Tsang's introduction of a set of profiles (indicators) derived under the DC framework in Tsang (\citeyear{tsang2015profiling}) and (\citeyear{tsang2017profiling})).

A well-known fact about analysing financial time series is that the source of many of the challenges can be traced back to the use of physical time (see Dacarogna et al. (\citeyear{genccay2001introduction})). Aloud et al. (\citeyear{aloud2012directional}) discussed the potential of studying financial time series using the DC framework (referred to as the DC approach in their paper) resides in its underlying `intrinsic time' paradigm. They pointed out that mapping financial time series from the physical time to event-based intrinsic time is the key to how the approach filters out irrelevant information and disturbance observed in the dataset and generates valuable market insights of our interests. Inspired by the studies of complex systems, Petrov et al. (\citeyear{petrov2018agent}) took a different route of demonstrating this point with the use of agent-based modelling. They created a market with trading agents that operate in event-based intrinsic time and found that the price movements generated under such conditions experience statistical properties we observed in real-world physical time FX markets. Such reproduction of real-world stylised facts is another indication of the intrinsic time mechanism being one of the contributing factors to the market dynamics. Recently, Glattfelder \& Golub (\citeyear{glattfelder2022bridging}) derived an analytical relationship between physical and intrinsic time based on the scaling laws. In particular, the expression they derived decomposes the movements of the physical-time time series into volatility and liquidity components expressed in intrinsic time. That allows us to explicitly characterise the dynamics observed in physical time using its intrinsic-time representation.

As DC intrinsic time framework becomes theoretically sound, applications building on the framework have been devised. Golub et al. (\citeyear{golub2016multi}) introduced the Intrinsic Network - an event-based framework based on directional changes. Combining the Intrinsic Network and information theory, they devised a liquidity measure that was shown to be able to predict market stress in terms of liquidity shocks. In Golub et al. (\citeyear{golub2018alpha}), the liquidity measure was integrated with other implementations derived from the DC framework and an algorithmic trading strategy called The Alpha Engine was introduced. The Alpha Engine has several interesting features. First, the bare-bones version of the model (without tweaking) has been shown to be robust, profitable, and can be implemented in real-time. Second, Alpha Engine provides liquidity in the market, i.e., it opens long positions when other market players intend to short and vice versa. The Alpha Engine thus contributes to the healthiness of the market as a participant. Third, Alpha Engine `beats' random walk processes - it is shown to be profitable even on price dynamics generated by a random walk. Within the context of volatility and risk management in finance, Petrov et al. (\citeyear{petrov2019instantaneous}) proposed an instantaneous volatility measure under the DC intrinsic time framework. They found seasonality patterns and long memory of volatility through empirical studies. Their work contributes not only to the development of practical tools but also to the understanding of the underlying stochastic drive of financial dynamics. Two further generalisations of the DC framework have been proposed. First, Petrov et al. (\citeyear{petrov2019intrinsic}) brought the framework into multidimensional space by extending the analytical expressions yielded from one-dimensional analyses to multidimensional space. Their methodology implies that previous works in one-dimensional space (analytical insights, empirical findings, and all the tools and implementations) can be extended to higher dimensions. Another generalisation was developed with respect to the types of stochastic processes (Mayerhofer (\citeyear{mayerhofer2019three})). These generalisations, as well as the advancements of the DC intrinsic time framework discussed previously, are indicative of the framework's promising potential worthy of further exploration.

In this chapter, we reviewed relevant works in two different realms: target transformations being used as an additional layer in modelling and the DC intrinsic time framework being a rising methodology. The literature surveyed demonstrates excellent potential in both research directions that justifies our curiosity in bringing them together. In the next chapter, we go into detail about the methodologies of combing the framework and target transformation in time series modelling.
