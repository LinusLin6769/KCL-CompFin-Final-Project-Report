\chapter{Literature Review}

Target transformation techniques have long been researched as part of the modelling process for over the past sixty years, and the same applies to the analyses of the underlying mechanisms in financial dynamics. This paper builts on both research directions and focuses on analysing the Directional Change (DC) intrinsic-time framework as a target transformation technique in time series forecasting. To the best of our knowledge as of this paper was written, we are not aware of any other attempt trying to bring these two methodologies together. In this chapter, we cover the most relevant works in both realms. We first review some target transformation deveplopments in Section \ref{sec: target transformation}, and we then look at some of the advancements in the Directional Change intrinsic-time framework in Section \ref{sec: DC}.

\section{Target Transformation}\label{sec: target transformation}

In most cases, time series analyses yielded from modelling are justified conditional on the assumptions made by the models with regard to the data. If the assumptions are not met by the original data, applying some sort of transformation (often non-linear) to the data can help generating these conditions. This has led to a variety of transformation techniques for these types of purposes. In this section, we go through some important assumptions made by the models and the corresponding transformations found in literature that we find most worthy of mentioning.

Homoscedasticity condition\footnotemark{} is one of the most common assumptions for models involving statistical inferences. As a reference to the analysis of variance, M. S. Bartlett (\citeyear{10.2307/3001536}) provided one of the earliest summaries of transformations on raw data addressing this. He covered parametric transformations used in stablising the variance of modelling error, especially for Poisson and Binomial distributed variables where the variance is a known function of the mean. He discussed, both theoratical and empirical, some of the optimal scales and families of transformations to choose from given different circumstances. His work showed that modelling tasks do benefit from suitable transformations.
\footnotetext{The homoscedasticity (or homogeneity of variances) condition requires the variances of different subsets of the sample to be the same. In the case of time series modelling, it's equivalent of requiring a constant variance throughout time.}

Another common assumption made by the models is normality. Almost all statistical inferences assume the variable of interest being normaly distributed, e.g., t-test, Analysis of Variance (ANOVA), linear regression etc. It therefore is of great interest if we have the opportunity to create such condition for the models to operate under. Box \& Cox (\citeyear{box1964analysis}) made a major contribution in this regard by proposing the well-known Box-Cox transformation. The Box-Cox transformation includes both power and logarithmic transformations. It aims at achieving normality of the observations and has been popular in developping modelling methodologies ever since. A good example is the method proposed in C. Bergmeir et al. (\citeyear{bergmeir2016bagging}). In combination with the widespreaded exponential smoothing (ETS) forecasting method and the bootstrap aggregation (bagging) technique in machine learning, they proposed a bagging exponential smoothing method using STL decomposition and Box-Cox transformation. The proposed method was tested on the M3 competition dataset and achieved better results than all the original M3 participants (see Makridakis et al. \citeyear{makridakis2000m3} for the M3 competition).

The method published by Box \& Cox, however, is only valid for positive real values. Modifications of the Box-Cox transformation have thus been proposed to address the problem. A major one is made by Bickel \& Doksum (\citeyear{bickel1981analysis}). They embedded a sign-function to the power transformation such that the transformation function covers the whole real line. Nevertheless, this modification has its shortcomings: it's shown by Yeo \& Johnson (\citeyear{10.1093/biomet/87.4.954}) that Bickel \& Doksum's modified version of the transformation handles skewed distribution poorly. Yeo \& Johnson pointed out the reason being the signed power transformation was designed primarily for handling kurtosis, thus losing its edge with regard to skewness. Following up, Yeo \& Johnson proposed a new version of the power transformation in the same puplication (\citeyear{10.1093/biomet/87.4.954}). Their transformation is a generalised version of the Box-Cox transformation and approximates normality while being well-defined on the real line and inducing appropriate symmetricity.

Until now, we looked at how a mathematician or statistician would apply response transformation techniques aiming at fostering mathematical and statistical conditions assumed by the models. Meeting these assumptions improves the robustness of the conclusions drawn by the modelling results. In general sense, one can say that the transformations help the models to get better at `learning' the problem such that they generate more robust outputs. In the upcoming paragraphs, we take on this perspective of treating the transformation techniques as helpers in terms of the learning process of the models, and look at some transformation techniques very different from what was covered previously.

Decomposition methods is one of the major category of techniques that can be considered as target transformation in helping the models learn when applied to the target variable. These methods decompose the mixture of information contained within the observation into patterns, trends, cycles, or other dynamics that are easier to model, i.e., easier for models to learn from. Also referred to as spectrum analysis, a particular good example would be the giant branch of studies on Fourier-styled transformations in time series modelling (see Kay \& Marple \citeyear{kay1981spectrum} and Bloomfield \citeyear{bloomfield2004fourier}). Typical Fourier methodologies build on transforming the observations, which are sampled from the time domain, into the frequency domain and decompose them into more informative signals. Out of the abundant history and advancements of spectrum analysis, we selectively provide a very brief overview of some relevant methodologies in the context of target transformation.

A novel type of methods build on what Fourier methods do and operates in the time-frequency domain, i.e., these methods come up with time-frequency representations of the observations. The empirical mode decomposition (EMD) propsed by Huang et al. (\citeyear{huang1998empirical}) is one of the key methodologies as such. EMD decomposes the original time series into so called `intrinsic mode functions' (IMF). The IMFs carry information of the underlying structures contained in the observations and can then be used for modelling tasks. The familty of wavelet transform methods constitutes another class of methods that operates in time-frequency domain (see Daubechies \citeyear{daubechies1992ten} and Percival \& Walden \citeyear{percival2000wavelet}). Shensa et al. \citeyear{shensa1992discrete} first proposed and provided a framework for the discrete wavelet transform (DWT), which belongs to the wavelet transform family. DWT filters the original time series in several folds and yields a denoised version of the observation. The information carried by the transformed time series is then more clear and preferably easier to learn. The rational of using such techniques as target transformation in time series modelling is to provide the models with more informative, e.g., less noisy, dataset to learn from. We hope that this extra procedure helps producing a better trained (learned) model.

There is another branch of decomposition methods that are more related to statistical approaches. These decomposition methods generally consider the time series as a mixture of three components: seasonal, trend and remainder. They are often used to filter different information contained within the time series (see Wang, Smith, \& Hyndman \citeyear{wang2006characteristic}). For a real-world example, monthly unemployment data are usually presented after removing the seasonality. The resulting time series is hence more indicative for the variation of the general economy instead of seasonal disturbance (see Chapter 3.2 in Hyndman \& Athanasopoulos \citeyear{forecastingprincipleandpractice}). The STL decomposition proposed by Cleveland et al. \citeyear{cleveland1990stl} has been a robust method. The abbriviation stands for Seasonal and Trend decomposition using Loess. STL considers a time series as a sum (additive) or product (multiplicative) of the seasonal, trend and remainder components. STL is flexible and applicable to many use cases as it can handle any type of seasonality. Its flexibility also resides in its allowance for the user to have control over the time-varying seasonal component and smoothness of the trend-cycle. The X-11 method and the Seasonal Extraction in ARIMA Time Series (SEATS) procedure are time series models that rely heavily on seasonal and trend decompositions. They have had many variants and favorred by official statistical agencies around the world (see Dagum \& Bianconcini \citeyear{dagum2016seasonal})\footnote{X-11 was originally developped by the US Census Bureau and SEATS was created by the Bank of Spain.}. One of the state-of-the-art variants of this family is the X-13ARIMA-SEATS method produced, distributed and maintained by the US Census Bureau (see U.S. Census Bureau \citeyear{x13arimaseatsmanual} and Monsell \& Blakely \citeyear{monsell2013x}). It inherits powerful features from X-11, SEATS, and ARIMA methodologies while specialising in seasonal adjustment in extensive time series modelling. The model is conveniently accessible online\footnotemark{}.
\footnotetext{A webpage demonstration of the model is accessible on \url{http://www.seasonal.website/}; the open-source implementation of the model can be found in the \verb+seasonal+ package in R; and a distributed version can be found on the US Census Bureau website \url{https://www.census.gov/data/software/x13as.X-13ARIMA-SEATS.html}.}

\section{Directional Change intrinsic-time framework}\label{sec: DC}


