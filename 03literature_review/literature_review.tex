\chapter{Literature Review}

Target transformation techniques have been researched as part of the modelling process over the past sixty years. The same applies to the analyses of the underlying mechanisms in financial dynamics. This paper builds on both research directions and analyses the Directional Change (DC) intrinsic time framework as a target transformation technique in time series forecasting. To the best of our knowledge, as this paper was written, we are unaware of any other attempt to bring these two methodologies together. In this chapter, we cover the most relevant works in both realms. We first review some target transformation developments in Section \ref{sec: target transformation} and then look at some of the advancements in the Directional Change intrinsic time framework in Section \ref{sec: DC}.

\section{Target Transformation}\label{sec: target transformation}

In most cases, time series analyses yielded from modelling are justified conditional on the assumptions made by the models about the data. If the assumptions are not met by the original data, applying some transformation (often non-linear) to the data can help generate these conditions. This has led to various transformation techniques for these types of purposes. In this section, we go through some important assumptions made by the models and the corresponding transformations found in the literature that we find most worthy of mentioning.

Homoscedasticity condition\footnote{The homoscedasticity (homogeneity of variances) condition requires the variances of different subsets of the sample to be the same. In the case of time series modelling, it is equivalent to requiring a constant variance throughout time.} is one of the most common assumptions for models involving statistical inferences. As a reference to the analysis of variance, M. S. Bartlett (\citeyear{10.2307/3001536}) provided one of the earliest summaries of transformations on raw data addressing this. He covered parametric transformations used in stabilising the variance of modelling error, especially for Poisson and Binomial distributed variables where the variance is a known function of the mean. He discussed, both theoretical and empirical, some of the optimal scales and families of transformations to choose from given different circumstances. His work showed that modelling tasks do benefit from suitable transformations.

Another common assumption made by the models is normality. Almost all statistical inferences assume the variable of interest is normally distributed, e.g., t-test, Analysis of Variance (ANOVA), linear regression etc. Therefore, it is of great interest if we have the opportunity to create such conditions for the models to operate under. Box \& Cox (\citeyear{box1964analysis}) made a major contribution in this regard by proposing the well-known Box-Cox transformation. The Box-Cox transformation includes both power and logarithmic transformations. It aims at achieving normality of the observations and has been popular in developing modelling methodologies ever since. A good example is a method proposed in C. Bergmeier et al. (\citeyear{bergmeir2016bagging}). Combined with the widespread exponential smoothing (ETS) forecasting method and the bootstrap aggregation (bagging) technique in machine learning, they proposed a bagging exponential smoothing method using STL decomposition and Box-Cox transformation. The proposed method was tested on the M3 competition dataset and achieved better results than all the original M3 participants (see Makridakis et al. \citeyear{makridakis2000m3} for the M3 competition).

The method published by Box \& Cox, however, is only valid for positive real values. Modifications of the Box-Cox transformation have thus been proposed to address the problem. A major one is made by Bickel \& Doksum (\citeyear{bickel1981analysis}). They embedded a sign function to the power transformation such that the transformation function covers the whole real line. Nevertheless, this modification has its shortcomings: it is shown by Yeo \& Johnson (\citeyear{10.1093/Biomet/87.4.954}) that Bickel \& Doksum's modified version of the transformation handles skewed distribution poorly. Yeo \& Johnson pointed out the reason being the signed power transformation was designed primarily for handling kurtosis, thus losing its edge concerning skewness. Following up, Yeo \& Johnson proposed a new version of the power transformation in the same publication (\citeyear{10.1093/Biomet/87.4.954}). Their transformation is a generalised version of the Box-Cox transformation and approximates normality while being well-defined on the real line and inducing appropriate symmetricity.

Until now, we looked at how a mathematician or statistician would apply response transformation techniques to foster mathematical and statistical conditions assumed by the models. Meeting these assumptions improves the robustness of the conclusions drawn by the modelling results. In a general sense, one can say that the transformations help the models to get better at `learning' the problem such that they generate more robust outputs. In the upcoming paragraphs, we take on this perspective of treating the transformation techniques as helpers in terms of the learning process of the models and look at some transformation techniques very different from what was covered previously.

Decomposition methods constitute a significant category of techniques that can be considered target transformation in helping the models learn when applied to the target variable. These methods decompose the mixture of information contained within the observation into patterns, trends, cycles, or other dynamics that are easier to model, i.e., easier for models to learn from. Also referred to as spectrum analysis, a particularly good example would be the giant branch of studies on Fourier-styled transformations in time series modelling (see Kay \& Marple \citeyear{kay1981spectrum} and Bloomfield \citeyear{bloomfield2004fourier}). Typical Fourier methodologies build on transforming the observations sampled from the time domain into the frequency domain and decomposing them into more informative signals. Out of the great history and advancements of spectrum analysis, we selectively provide a very brief overview of some relevant methodologies in the context of target transformation.

A novel type of method builds on what Fourier methods do and operates in the time-frequency domain, i.e., these methods come up with time-frequency representations of the observations. The empirical mode decomposition (EMD) proposed by Huang et al. (\citeyear{huang1998empirical}) is one of the key methodologies as such. EMD decomposes the original time series into `intrinsic mode functions' (IMF). The IMF's carry information of the underlying structures contained in the observations and can then be used for modelling tasks. The family of wavelet transform methods constitutes another class of methods that operates in the time-frequency domain (see Daubechies \citeyear{daubechies1992ten} and Percival \& Walden \citeyear{percival2000wavelet}). Shensa et al. \citeyear{shensa1992discrete} first proposed and provided a framework for the discrete wavelet transform (DWT), which belongs to the wavelet transform family. DWT filters the original time series in several folds and yields a denoised version of the observation. The information carried by the transformed time series is then more clear and preferably easier to learn. The rationale of using such techniques as target transformation in time series modelling is to provide the models with a more informative, e.g., less noisy, dataset to learn from. We hope this extra procedure helps produce a better trained (learned) model.

Another branch of decomposition methods is more related to statistical approaches. These decomposition methods generally consider the time series as a mixture of three components: seasonal, trend and remainder. They are often used to filter different information contained within the time series (see Wang, Smith, \& Hyndman \citeyear{wang2006characteristic}). For a real-world example, monthly unemployment data are usually presented after removing the seasonality. The resulting time series is hence more indicative of the variation of the general economy instead of seasonal disturbance (see Chapter 3.2 in Hyndman \& Athanasopoulos \citeyear{forecastingprincipleandpractice}). The STL decomposition proposed by Cleveland et al. \citeyear{cleveland1990stl} has been a robust method. The abbreviation stands for Seasonal and Trend decomposition using Loess. STL considers a time series as a sum (additive) or product (multiplicative) of the seasonal, trend and remainder components. STL is flexible and applicable to many use cases as it can handle any type of seasonality. Its flexibility also resides in its allowance for the user to have control over the time-varying seasonal component and smoothness of the trend cycle. The X-11 method and the Seasonal Extraction in ARIMA Time Series (SEATS) procedure are time series models that rely heavily on seasonal and trend decompositions. They have had many variants and are favoured by official statistical agencies around the world (see Dagum \& Bianconcini \citeyear{dagum2016seasonal})\footnote{X-11 was initially developed by the US Census Bureau, and SEATS was created by the Bank of Spain.}. One of the state-of-the-art variants of this family is the X-13ARIMA-SEATS method produced, distributed and maintained by the US Census Bureau (see US Census Bureau \citeyear{x13arimaseatsmanual} and Monsell \& Blakely \citeyear{monsell2013x}). It inherits powerful features from X-11, SEATS, and ARIMA methodologies while specialising in seasonal adjustment in extensive time series modelling. The model is conveniently accessible online\footnotemark{}.
\footnotetext{A webpage demonstration of the model is accessible on \url{http://www.seasonal.website/}; the open-source implementation of the model can be found in the \verb+seasonal+ package in R, and a distributed version can be found in the US Census Bureau website \url{https://www.census.gov/data/software/x13as.X-13ARIMA-SEATS.html}.}

\section{Directional Change intrinsic time framework}\label{sec: DC}

The technical core of the Directional Change (DC) intrinsic time framework is quite simple; it is an algorithm (the DC dissection) that samples a time series and yields a new time series, which is the subset of the orginal observations. By analysing the properties of the resulting time series, it has been found that despite the simplicity of this algorithm, it provides powerful perspectives of looking at market dynamics in the time domain. In this chapter, we first look at critical works contributing to the advancements of the DC intrinsic time framework. Then we move on to cover some applications that further develop the value of the framework by trying to harness its potential.

As the developments of many frameworks, the DC intrinsic time framework is a collection of elements gathered from multiple works over time. Guillaume et al. (\citeyear{guillaume1997bird}) first published the Directional Change dissection algorithm part of the framework. The algorithm is presented and used to generate a set of measurements (statistics, variables), from which the authors presented a set of stylised facts found empirically in the spot intra-daily foreign exchange (FX) markets. These stylised facts shed new light on our understanding about market dynamics, especially concerning micro-structure topics, including time-heterogeneity, price formation, market efficiency, liquidity, and both the modelling and the learning process of the market. A little more than a decade later, Glattfelder et al. (\citeyear{glattfelder2011patterns}) discovered a set of twelve scaling laws derived from the DC sampling algorithm. The discovery of the laws added theoretical foundation to the DC dissection algorithm, because the output time series not only carry qualitative information (stylised facts) but also interesting quantitative properties. As the DC dissection algorithm's ability to extract quantitative information has been studied, it has given rise to the methodology becoming a framework.

Many investigations into how the DC framework works have been doen in the recent dacade. Aloud et al. (\citeyear{aloud2012directional}) discussed the potential of studying financial time series using the DC framework (refered to as the DC approach in their paper) resides in its underlying `intrinsic time' component. They pointed out that mapping financial time series from the physical time to event-based intrinsic time is the key of how the approach filters out irrelevant information and disturbance observed in the dataset and generates valuable market insights of our interests. And that indeed matches what we know about anlysing financial time series: the source of many the challenges can be traced back to the use of physical time (see \citeyear{genccay2001introduction}). Petrov et al. (\citeyear{petrov2018agent}) took a different route of demonstrating this point with the use of agent-based modelling. They created a market with trading agents that operate in event-based intrinsic time and found that the price movements generated under such conditions experience statistical properties we observed in real-world physical time FX markets. Such reproduction of real-world stylised facts is another indication of the intrinsic time mechanism being one of the contributing factors to the market dynamics. 


The fact that the DC dissection algorithm is able to extract qualitative and quantitative information has given rise to the methodology becoming a framework. 



The Directional Change dissection algorithm of the framework was the first element to be published 

As the development of a lot of the frameworks, the DC intrinsic time framework did not start out as a `framework', so to speak. It took

Upon its debut, the DC intrinsic time framework did not start out as a `framework', so to speak. 

Guillaume et al. \citeyear{guillaume1997bird} is the publication where the DC intrinsic time framework was first presented. Upon the framework's first appearance, it's yet a `framework', so to speak, because it simply was an sampling algorithm (the technical core mentioned in the previous paragraph). In this paper, the algorithm was proposed and used to generate a set of measurements (variables), from which the authors presented a set of stylised facts found empirically in the spot intra-daily foreign exchange (FX) markets. These stylised facts shed new light on market dynamics, especially concerning market micro-structure topics, including time-heterogeneity, price formation, market efficiency, liquidity, and both the modelling and learning process of the market. These stylised facts constitute an important part that later gave rise to the DC framework.

A little more than a decade later, Glattfelder et al. \citeyear{glattfelder2011patterns} discovered a set of twelve scaling laws derived from the DC sampling algorithm. The discovery of these laws constitutes the theoratical foundation of the DC algorithm. 

established a fine, though perhaps not complete, theoratical foundation of the DC algorithm's capability in discerning 
